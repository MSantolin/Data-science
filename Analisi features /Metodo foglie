import psycopg2
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from collections import Counter
from itertools import combinations

# ============================
# 1. Caricamento dei dati
# ============================
host = "localhost"
port = "5432"
dbname = "DataScience"
user = "postgres"
password = "2430"

conn = psycopg2.connect(host=host, port=port, dbname=dbname, user=user, password=password)
query = "SELECT * FROM public.vew_mts_prov"
df = pd.read_sql_query(query, conn)
conn.close()

print("Dati caricati:")
print(df.head())

# ================================
# 2. Preprocessing e Feature Engineering
# ================================
df['giorno'] = pd.to_datetime(df['giorno'])
df = df.sort_values(['provincia', 'giorno']).reset_index(drop=True)
df['giorno_della_settimana'] = df['giorno'].dt.weekday
df['mese'] = df['giorno'].dt.month

# Creazione di lag features (da 1 a 7 giorni)
for lag in range(1, 8):
    df[f'lag_{lag}'] = df.groupby('provincia')['total_transiti'].shift(lag)

df['target'] = df['total_transiti']
df['provincia_orig'] = df['provincia']

df = df.dropna().reset_index(drop=True)

# One-Hot Encoding per la provincia
df = pd.get_dummies(df, columns=['provincia'], prefix='provincia')

lag_features = [f'lag_{i}' for i in range(1, 8)]
temporal_features = ['giorno_della_settimana', 'mese']
provincia_columns = [col for col in df.columns if col.startswith('provincia_') and col != 'provincia_orig']
feature_columns = lag_features + temporal_features + provincia_columns

X = df[feature_columns]
y = df['target']

# ================================
# 3. Suddivisione in Train e Test (time-aware) per provincia
# ================================
train_dfs = []
test_dfs = []

for prov in df['provincia_orig'].unique():
    df_prov = df[df['provincia_orig'] == prov].copy().sort_values('giorno')
    split_idx = int(len(df_prov) * 0.8)
    train_dfs.append(df_prov.iloc[:split_idx])
    test_dfs.append(df_prov.iloc[split_idx:])

train_df = pd.concat(train_dfs).sort_index()
test_df = pd.concat(test_dfs).sort_index()

X_train = train_df[feature_columns]
y_train = train_df['target']
dates_train = train_df['giorno']
province_train = train_df['provincia_orig']

X_test = test_df[feature_columns]
y_test = test_df['target']
dates_test = test_df['giorno']
province_test = test_df['provincia_orig']

print("Province nel test set:", province_test.unique())

# ================================
# 4. Scaling delle Feature e del Target
# ================================
scaler_X = MinMaxScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()
y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()

# ================================
# 5. Allenamento del Modello
# ================================
model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)
model.fit(X_train_scaled, y_train_scaled)

# Previsioni sul set di test
y_pred_scaled = model.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()

# Calcolo del MAE
mae_overall = mean_absolute_error(y_test, y_pred)
print(f'MAE complessivo sul test set: {mae_overall}')

# Calcolo del MAE per provincia
results_test = pd.DataFrame({
    'giorno': dates_test.values,
    'provincia': province_test.values,
    'true_traffic': y_test.values,
    'predicted_traffic': y_pred
})

mae_per_provincia = results_test.groupby('provincia').apply(
    lambda grp: mean_absolute_error(grp['true_traffic'], grp['predicted_traffic'])
)

print("MAE per provincia:")
print(mae_per_provincia)

# Grafico di confronto per una provincia di esempio
example_prov = results_test['provincia'].unique()[0]
results_example = results_test[results_test['provincia'] == example_prov]

plt.figure(figsize=(10, 6))
plt.plot(results_example['giorno'], results_example['true_traffic'], label='Reale')
plt.plot(results_example['giorno'], results_example['predicted_traffic'], label='Predetto', linestyle='--')
plt.xlabel('Giorno')
plt.ylabel('Traffico Totale')
plt.title(f'Confronto traffico per la provincia di {example_prov}')
plt.legend()
plt.show()

# ================================
# 6. Analisi delle coppie di postazioni nei sottoinsiemi delle foglie
# ================================
leaf_to_postazione = {}

for tree_id, tree in enumerate(model.estimators_):
    leaf_indices = tree.apply(X_train_scaled)
    
    for record_id, leaf_id in enumerate(leaf_indices):
        postazione_id = train_df.iloc[record_id]['postazione']
        if (tree_id, leaf_id) not in leaf_to_postazione:
            leaf_to_postazione[(tree_id, leaf_id)] = []
        leaf_to_postazione[(tree_id, leaf_id)].append(postazione_id)

# Conta le coppie di postazioni che appaiono insieme nelle foglie finali
postazione_pairs = []
for records in leaf_to_postazione.values():
    postazione_pairs.extend(combinations(records, 2))

pair_counts = Counter(postazione_pairs)
most_common_pairs = pair_counts.most_common(20)

print("\nCoppie di postazioni più comuni nelle foglie finali:")
for pair, count in most_common_pairs:
    print(f"{pair}: {count} occorrenze")

# Creazione del DataFrame per la visualizzazione
top_pairs_df = pd.DataFrame(most_common_pairs, columns=['Pair', 'Count'])

plt.figure(figsize=(10, 6))
plt.barh([str(pair) for pair in top_pairs_df['Pair']], top_pairs_df['Count'], color='skyblue')
plt.xlabel("Numero di occorrenze")
plt.ylabel("Coppie di postazioni")
plt.title("Coppie di postazioni più comuni nei sottoinsiemi delle foglie")
plt.gca().invert_yaxis()
plt.show()


